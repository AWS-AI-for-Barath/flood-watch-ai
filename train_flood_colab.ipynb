{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåä FloodWatch AI ‚Äî High-Accuracy YOLO Training (v2)\n",
    "\n",
    "This notebook trains a **high-accuracy flood-aware YOLOv8s** model on GPU with an expanded dataset,\n",
    "quality-audited labels, class balancing, and enhanced augmentations.\n",
    "\n",
    "**Upgrades from v1:**\n",
    "| Setting | Baseline (v1) | High-Accuracy (v2) |\n",
    "| --- | --- | --- |\n",
    "| Model | YOLOv8**n** | YOLOv8**s** (or **m**) |\n",
    "| Resolution | 640px | **768px** |\n",
    "| Epochs | 40 | **100** |\n",
    "| Early Stopping | patience=10 | patience=**20** |\n",
    "| Augmentations | Default | **Mosaic + HSV + Flip + Scale + Mixup** |\n",
    "| Dataset | ~781 images | **‚â•1200 images** (expanded + balanced) |\n",
    "| Labels | Raw | **Audited & cleaned** |\n",
    "\n",
    "**Target:** mAP50 ‚â• 0.80 (baseline: 0.69)\n",
    "\n",
    "> ‚ö° **Runtime:** Go to `Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)` before running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics roboflow Pillow\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download & Expand Flood Dataset\n",
    "\n",
    "Downloads the primary flood dataset and additional sources to reach ‚â•1200 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import os, glob, shutil\n",
    "\n",
    "API_KEY = \"yUaG6RQfZ6ZrAFlwj6w7\"\n",
    "\n",
    "# ‚îÄ‚îÄ Primary dataset: yolo-floods-relief (781 images) ‚îÄ‚îÄ\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "project = rf.workspace(\"modellabel\").project(\"yolo-floods-relief\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\", location=\"./flood_dataset\")\n",
    "print(\"‚úÖ Primary flood dataset downloaded!\")\n",
    "\n",
    "# Count current images\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    imgs = glob.glob(f'./flood_dataset/{split}/images/*')\n",
    "    print(f'  {split}: {len(imgs)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Download additional flood datasets to expand beyond 781 ‚îÄ‚îÄ\n",
    "# Add more Roboflow public flood datasets here\n",
    "ADDITIONAL_DATASETS = [\n",
    "    # (workspace, project, version, description)\n",
    "    # Uncomment and add datasets you have access to:\n",
    "    # (\"workspace-name\", \"flood-detection-project\", 1, \"FloodNet subset\"),\n",
    "]\n",
    "\n",
    "for ws, proj, ver, desc in ADDITIONAL_DATASETS:\n",
    "    print(f'Downloading: {desc}...')\n",
    "    try:\n",
    "        p = rf.workspace(ws).project(proj)\n",
    "        v = p.version(ver)\n",
    "        v.download('yolov8', location=f'./_extra_{proj}_v{ver}')\n",
    "        \n",
    "        # Merge into primary dataset\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            src_imgs = f'./_extra_{proj}_v{ver}/{split}/images'\n",
    "            src_lbls = f'./_extra_{proj}_v{ver}/{split}/labels'\n",
    "            if os.path.isdir(src_imgs):\n",
    "                dst_imgs = f'./flood_dataset/{split}/images'\n",
    "                dst_lbls = f'./flood_dataset/{split}/labels'\n",
    "                for f in os.listdir(src_imgs):\n",
    "                    if not os.path.exists(os.path.join(dst_imgs, f)):\n",
    "                        shutil.copy2(os.path.join(src_imgs, f), dst_imgs)\n",
    "                if os.path.isdir(src_lbls):\n",
    "                    for f in os.listdir(src_lbls):\n",
    "                        if not os.path.exists(os.path.join(dst_lbls, f)):\n",
    "                            shutil.copy2(os.path.join(src_lbls, f), dst_lbls)\n",
    "        print(f'  ‚úÖ {desc} merged')\n",
    "    except Exception as e:\n",
    "        print(f'  ‚ùå Failed: {e}')\n",
    "\n",
    "# Final count\n",
    "total = 0\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    imgs = glob.glob(f'./flood_dataset/{split}/images/*')\n",
    "    total += len(imgs)\n",
    "    print(f'  {split}: {len(imgs)} images')\n",
    "print(f'  Total: {total} images')\n",
    "if total >= 1200:\n",
    "    print('‚úÖ Target ‚â•1200 reached!')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è Need {1200 - total} more images. Add extra datasets above or upload manually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ Upload Additional Images (Optional)\n",
    "\n",
    "If you have extra flood images, upload them to Colab and merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upload and merge extra images from Google Drive or local files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# EXTRA_DIR = '/content/drive/MyDrive/flood_extra_images'\n",
    "#\n",
    "# if os.path.isdir(EXTRA_DIR):\n",
    "#     dst = './flood_dataset/train/images'\n",
    "#     for f in os.listdir(EXTRA_DIR):\n",
    "#         if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "#             shutil.copy2(os.path.join(EXTRA_DIR, f), dst)\n",
    "#     print(f'Extra images merged from {EXTRA_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Normalize Labels (Polygon ‚Üí BBox)\n",
    "\n",
    "**Critical step:** Some flood datasets use segmentation polygon labels.\n",
    "YOLO detection training requires exactly 5 values per line: `class x_center y_center width height`.\n",
    "This cell converts all polygon annotations to bounding box equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_to_bbox(parts):\n",
    "    \"\"\"Convert polygon annotation to YOLO bbox format.\"\"\"\n",
    "    if len(parts) < 5: return None\n",
    "    try:\n",
    "        cls_id = int(parts[0])\n",
    "        coords = [float(v) for v in parts[1:]]\n",
    "    except (ValueError, IndexError): return None\n",
    "    if len(coords) % 2 != 0: return None\n",
    "    xs, ys = coords[0::2], coords[1::2]\n",
    "    if not xs or not ys: return None\n",
    "    x_min, x_max = max(0, min(xs)), min(1, max(xs))\n",
    "    y_min, y_max = max(0, min(ys)), min(1, max(ys))\n",
    "    w, h = x_max - x_min, y_max - y_min\n",
    "    if w <= 0 or h <= 0: return None\n",
    "    return f\"{cls_id} {x_min+w/2:.6f} {y_min+h/2:.6f} {w:.6f} {h:.6f}\"\n",
    "\n",
    "poly_count = 0\n",
    "bbox_count = 0\n",
    "files_fixed = 0\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    lbl_dir = f\"./flood_dataset/{split}/labels\"\n",
    "    if not os.path.isdir(lbl_dir): continue\n",
    "    for fname in sorted(os.listdir(lbl_dir)):\n",
    "        if not fname.endswith(\".txt\"): continue\n",
    "        fpath = os.path.join(lbl_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            lines = f.readlines()\n",
    "        converted = []\n",
    "        had_polygon = False\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                converted.append(line.strip())\n",
    "                bbox_count += 1\n",
    "            elif len(parts) > 5:\n",
    "                bbox_line = polygon_to_bbox(parts)\n",
    "                if bbox_line:\n",
    "                    converted.append(bbox_line)\n",
    "                    poly_count += 1\n",
    "                    had_polygon = True\n",
    "        if had_polygon:\n",
    "            with open(fpath, \"w\") as f:\n",
    "                f.write(\"\\n\".join(converted) + \"\\n\")\n",
    "            files_fixed += 1\n",
    "\n",
    "print(f\"Polygons converted to bbox: {poly_count}\")\n",
    "print(f\"Already bbox: {bbox_count}\")\n",
    "print(f\"Files fixed: {files_fixed}\")\n",
    "print(\"\\u2705 All labels are now in detection bbox format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Audit & Clean Labels\n",
    "\n",
    "Automatically detect and fix annotation quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "VALID_CLASS_IDS = {0, 1, 2, 3, 4, 5}\n",
    "MIN_BOX_AREA = 0.001\n",
    "DUPLICATE_IOU_THRESHOLD = 0.95\n",
    "\n",
    "def compute_iou(b1, b2):\n",
    "    \"\"\"IoU between two YOLO boxes (cls, xc, yc, w, h).\"\"\"\n",
    "    _, x1, y1, w1, h1 = b1\n",
    "    _, x2, y2, w2, h2 = b2\n",
    "    ax1, ay1, ax2, ay2 = x1-w1/2, y1-h1/2, x1+w1/2, y1+h1/2\n",
    "    bx1, by1, bx2, by2 = x2-w2/2, y2-h2/2, x2+w2/2, y2+h2/2\n",
    "    ix1, iy1, ix2, iy2 = max(ax1,bx1), max(ay1,by1), min(ax2,bx2), min(ay2,by2)\n",
    "    if ix2<=ix1 or iy2<=iy1: return 0.0\n",
    "    inter = (ix2-ix1)*(iy2-iy1)\n",
    "    return inter / (w1*h1 + w2*h2 - inter) if (w1*h1+w2*h2-inter)>0 else 0.0\n",
    "\n",
    "def audit_and_fix(dataset_dir, fix=True):\n",
    "    stats = defaultdict(int)\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        lbl_dir = os.path.join(dataset_dir, split, 'labels')\n",
    "        if not os.path.isdir(lbl_dir): continue\n",
    "        for fname in sorted(os.listdir(lbl_dir)):\n",
    "            if not fname.endswith('.txt'): continue\n",
    "            fpath = os.path.join(lbl_dir, fname)\n",
    "            with open(fpath) as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            anns = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5: continue\n",
    "                try:\n",
    "                    cls, xc, yc, w, h = int(parts[0]), *[float(p) for p in parts[1:5]]\n",
    "                except: continue\n",
    "\n",
    "                if cls not in VALID_CLASS_IDS:\n",
    "                    stats['invalid_class'] += 1; continue\n",
    "                xc = max(0, min(1, xc)); yc = max(0, min(1, yc))\n",
    "                if xc-w/2 < 0: w = xc*2\n",
    "                if xc+w/2 > 1: w = (1-xc)*2\n",
    "                if yc-h/2 < 0: h = yc*2\n",
    "                if yc+h/2 > 1: h = (1-yc)*2\n",
    "                if w*h < MIN_BOX_AREA:\n",
    "                    stats['tiny_box'] += 1; continue\n",
    "                anns.append((cls, xc, yc, w, h))\n",
    "            \n",
    "            # Remove duplicates\n",
    "            final = []\n",
    "            for a in anns:\n",
    "                dup = any(a[0]==e[0] and compute_iou(a,e) > DUPLICATE_IOU_THRESHOLD for e in final)\n",
    "                if dup: stats['duplicate'] += 1\n",
    "                else: final.append(a)\n",
    "            \n",
    "            if not final:\n",
    "                stats['empty_label'] += 1\n",
    "                if fix:\n",
    "                    os.remove(fpath)\n",
    "                    for ext in ('.jpg','.jpeg','.png'):\n",
    "                        img = fpath.replace('/labels/','/images/').replace('.txt', ext)\n",
    "                        if os.path.isfile(img): os.remove(img); break\n",
    "                continue\n",
    "            \n",
    "            if fix and len(final) != len(lines):\n",
    "                with open(fpath, 'w') as f:\n",
    "                    for cls, xc, yc, w, h in final:\n",
    "                        f.write(f'{cls} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\\n')\n",
    "                stats['files_fixed'] += 1\n",
    "    return dict(stats)\n",
    "\n",
    "# Run audit\n",
    "report = audit_and_fix('./flood_dataset', fix=True)\n",
    "print('\\nüìã Label Audit Report:')\n",
    "for k, v in sorted(report.items()):\n",
    "    print(f'  {k:20s}: {v}')\n",
    "print('‚úÖ Labels cleaned!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Balance Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "CLASS_NAMES = {0:'person', 1:'car', 2:'bicycle', 3:'motorcycle', 4:'bus', 5:'truck'}\n",
    "\n",
    "def analyze_classes(dataset_dir, split='train'):\n",
    "    counts = Counter()\n",
    "    imgs_per_cls = defaultdict(set)\n",
    "    lbl_dir = os.path.join(dataset_dir, split, 'labels')\n",
    "    if not os.path.isdir(lbl_dir): return counts, imgs_per_cls\n",
    "    for fname in os.listdir(lbl_dir):\n",
    "        if not fname.endswith('.txt'): continue\n",
    "        with open(os.path.join(lbl_dir, fname)) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    try:\n",
    "                        cls = int(parts[0])\n",
    "                        counts[cls] += 1\n",
    "                        imgs_per_cls[cls].add(fname)\n",
    "                    except: pass\n",
    "    return counts, imgs_per_cls\n",
    "\n",
    "counts, imgs_per_cls = analyze_classes('./flood_dataset')\n",
    "print('üìä Class Distribution (Before):')\n",
    "for cls_id in sorted(counts):\n",
    "    name = CLASS_NAMES.get(cls_id, f'cls_{cls_id}')\n",
    "    print(f'  {cls_id} ({name:12s}): {counts[cls_id]:5d} annotations, {len(imgs_per_cls[cls_id]):4d} images')\n",
    "\n",
    "# Detect and fix imbalance\n",
    "if counts:\n",
    "    median = sorted(counts.values())[len(counts)//2]\n",
    "    minorities = {c: v for c, v in counts.items() if v < median * 0.5}\n",
    "    if minorities:\n",
    "        print(f'\\n‚ö†Ô∏è Minority classes detected, oversampling...')\n",
    "        img_dir = './flood_dataset/train/images'\n",
    "        lbl_dir = './flood_dataset/train/labels'\n",
    "        dup_count = 0\n",
    "        for cls_id, cls_imgs in imgs_per_cls.items():\n",
    "            if cls_id not in minorities: continue\n",
    "            for label_fname in list(cls_imgs):\n",
    "                base = os.path.splitext(label_fname)[0]\n",
    "                # Duplicate\n",
    "                new_lbl = os.path.join(lbl_dir, f'{base}_dup.txt')\n",
    "                if not os.path.exists(new_lbl):\n",
    "                    shutil.copy2(os.path.join(lbl_dir, label_fname), new_lbl)\n",
    "                    for ext in ('.jpg','.jpeg','.png'):\n",
    "                        src = os.path.join(img_dir, base + ext)\n",
    "                        if os.path.isfile(src):\n",
    "                            shutil.copy2(src, os.path.join(img_dir, f'{base}_dup{ext}'))\n",
    "                            dup_count += 1; break\n",
    "                # Horizontal flip\n",
    "                flip_lbl = os.path.join(lbl_dir, f'{base}_flip.txt')\n",
    "                if not os.path.exists(flip_lbl):\n",
    "                    with open(os.path.join(lbl_dir, label_fname)) as f:\n",
    "                        flipped = []\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 5:\n",
    "                                parts[1] = str(round(1.0 - float(parts[1]), 6))\n",
    "                                flipped.append(' '.join(parts))\n",
    "                    with open(flip_lbl, 'w') as f:\n",
    "                        f.write('\\n'.join(flipped) + '\\n')\n",
    "                    for ext in ('.jpg','.jpeg','.png'):\n",
    "                        src = os.path.join(img_dir, base + ext)\n",
    "                        if os.path.isfile(src):\n",
    "                            img = Image.open(src)\n",
    "                            ImageOps.mirror(img).save(os.path.join(img_dir, f'{base}_flip{ext}'))\n",
    "                            dup_count += 1; break\n",
    "        print(f'  Added {dup_count} augmented images')\n",
    "        counts2, _ = analyze_classes('./flood_dataset')\n",
    "        print('\\nüìä Class Distribution (After):')\n",
    "        for cls_id in sorted(counts2):\n",
    "            name = CLASS_NAMES.get(cls_id, f'cls_{cls_id}')\n",
    "            print(f'  {cls_id} ({name:12s}): {counts2[cls_id]:5d} annotations')\n",
    "    else:\n",
    "        print('\\n‚úÖ Classes are balanced!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Prepare Dataset Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Write clean dataset YAML\n",
    "DATA_YAML = './flood_dataset.yaml'\n",
    "\n",
    "config = {\n",
    "    'path': os.path.abspath('./flood_dataset'),\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'names': {\n",
    "        0: 'person',\n",
    "        1: 'car',\n",
    "        2: 'bicycle',\n",
    "        3: 'motorcycle',\n",
    "        4: 'bus',\n",
    "        5: 'truck',\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(DATA_YAML, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print('Dataset config:')\n",
    "with open(DATA_YAML) as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Final image counts\n",
    "total = 0\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    n = len(glob.glob(f'./flood_dataset/{split}/images/*'))\n",
    "    total += n\n",
    "    print(f'{split}: {n} images')\n",
    "print(f'Total: {total} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Choose Model Size\n",
    "\n",
    "Select based on your GPU memory:\n",
    "- **T4 (15 GB):** `yolov8s.pt` at 768px ‚úÖ\n",
    "- **A100/V100 (40+ GB):** `yolov8m.pt` at 832px üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê MODEL SELECTION ‚ïê‚ïê‚ïê\n",
    "# Change these based on your GPU:\n",
    "MODEL_NAME = 'yolov8s.pt'   # Options: 'yolov8s.pt', 'yolov8m.pt'\n",
    "IMG_SIZE = 768               # Options: 768, 832\n",
    "EPOCHS = 100\n",
    "PATIENCE = 20\n",
    "\n",
    "# Auto-detect GPU and suggest settings\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f'GPU Memory: {gpu_mem:.1f} GB')\n",
    "    if gpu_mem >= 30:\n",
    "        print('üí° You can use yolov8m.pt at 832px!')\n",
    "    elif gpu_mem >= 14:\n",
    "        print('üí° yolov8s.pt at 768px is optimal for your GPU')\n",
    "    else:\n",
    "        print('üí° Consider yolov8s.pt at 640px for lower VRAM')\n",
    "        IMG_SIZE = 640\n",
    "\n",
    "print(f'\\nSelected: {MODEL_NAME} @ {IMG_SIZE}px for {EPOCHS} epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train High-Accuracy Flood Model üöÄ\n",
    "\n",
    "Full training with enhanced augmentations and optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == YOLOv8m Maximum-Accuracy Training ==\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"yolov8m.pt\"\n",
    "IMG_SIZE = 832\n",
    "EPOCHS = 200\n",
    "PATIENCE = 40\n",
    "BATCH = -1  # auto-fit GPU memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vram = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({vram:.1f} GB)\")\n",
    "    if vram >= 24:\n",
    "        MODEL_NAME = \"yolov8l.pt\"\n",
    "        print(f\"Using {MODEL_NAME} (large VRAM)\")\n",
    "\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# Bbox tightening (shrink loose boxes 3%)\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    lbl_dir = f\"./flood_dataset/{split}/labels\"\n",
    "    if not os.path.isdir(lbl_dir): continue\n",
    "    for fname in sorted(os.listdir(lbl_dir)):\n",
    "        if not fname.endswith(\".txt\"): continue\n",
    "        fpath = os.path.join(lbl_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            lines = f.readlines()\n",
    "        out = []\n",
    "        for line in lines:\n",
    "            p = line.strip().split()\n",
    "            if len(p) != 5: out.append(line.strip()); continue\n",
    "            try:\n",
    "                c = int(p[0])\n",
    "                xc, yc, w, h = [float(x) for x in p[1:5]]\n",
    "                w *= 0.94; h *= 0.94\n",
    "                w = max(w, 0.01); h = max(h, 0.01)\n",
    "                out.append(f\"{c} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "            except: out.append(line.strip())\n",
    "        with open(fpath, \"w\") as f:\n",
    "            f.write(\"\\n\".join(out) + \"\\n\")\n",
    "print(\"Bboxes tightened\")\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data=\"./flood_dataset/data.yaml\",\n",
    "    epochs=EPOCHS, imgsz=IMG_SIZE, batch=BATCH,\n",
    "    patience=PATIENCE, device=0,\n",
    "    project=\"./runs\", name=\"flood_maxacc\", exist_ok=True, pretrained=True,\n",
    "    optimizer=\"AdamW\", lr0=0.002, lrf=0.01, weight_decay=0.0005,\n",
    "    warmup_epochs=5, cos_lr=True,\n",
    "    mosaic=1.0, mixup=0.1, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "    scale=0.5, translate=0.1, perspective=0.0005,\n",
    "    flipud=0.2, fliplr=0.5, degrees=5.0, shear=2.0,\n",
    "    copy_paste=0.1, erasing=0.1,\n",
    "    box=7.5, cls=0.5, dfl=1.5,\n",
    "    save=True, save_period=25, plots=True, val=True, verbose=True,\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluate & Compare with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "RUN_DIR = 'runs/detect/yolov8_flood_highacc'\n",
    "\n",
    "# Training curves\n",
    "results_img = glob.glob(f'{RUN_DIR}/results.png')\n",
    "if results_img:\n",
    "    print('üìà Training Curves:')\n",
    "    display(Image(filename=results_img[0], width=900))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_img = glob.glob(f'{RUN_DIR}/confusion_matrix.png')\n",
    "if cm_img:\n",
    "    print('\\nüìä Confusion Matrix:')\n",
    "    display(Image(filename=cm_img[0], width=600))\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_norm = glob.glob(f'{RUN_DIR}/confusion_matrix_normalized.png')\n",
    "if cm_norm:\n",
    "    display(Image(filename=cm_norm[0], width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Evaluate & Compare ==\n",
    "BASELINE_MAP50 = 0.69\n",
    "\n",
    "metrics = results.results_dict if hasattr(results, \"results_dict\") else {}\n",
    "p = metrics.get(\"metrics/precision(B)\", None)\n",
    "r = metrics.get(\"metrics/recall(B)\", None)\n",
    "m50 = metrics.get(\"metrics/mAP50(B)\", None)\n",
    "m95 = metrics.get(\"metrics/mAP50-95(B)\", None)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FloodWatch YOLO - Max-Accuracy Results\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Model: {MODEL_NAME} | Resolution: {IMG_SIZE}px | Epochs: {EPOCHS}\")\n",
    "print(f\"Precision:  {p}\")\n",
    "print(f\"Recall:     {r}\")\n",
    "print(f\"mAP50:      {m50}\")\n",
    "print(f\"mAP50-95:   {m95}\")\n",
    "if m50 is not None:\n",
    "    delta = m50 - BASELINE_MAP50\n",
    "    status = \"IMPROVED\" if delta > 0 else \"REGRESSION\"\n",
    "    print(f\"Baseline mAP50: {BASELINE_MAP50:.4f} | Delta: {delta:+.4f} | {status}\")\n",
    "print(\"=\" * 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on validation images\n",
    "val_images = glob.glob('./flood_dataset/valid/images/*')\n",
    "\n",
    "if val_images:\n",
    "    preds = best_model.predict(\n",
    "        source=val_images[:6],\n",
    "        save=True,\n",
    "        conf=0.4,\n",
    "        project='runs/detect',\n",
    "        name='flood_highacc_predictions',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    pred_imgs = sorted(glob.glob('runs/detect/flood_highacc_predictions/*.jpg'))\n",
    "    for img in pred_imgs[:6]:\n",
    "        display(Image(filename=img, width=500))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Export Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "best_pt = f'{RUN_DIR}/weights/best.pt'\n",
    "output_pt = 'yolov8_flood_highacc.pt'\n",
    "shutil.copy2(best_pt, output_pt)\n",
    "\n",
    "print(f'Model size: {os.path.getsize(output_pt) / 1024 / 1024:.1f} MB')\n",
    "print(f'Precision:  {metrics.box.mp:.4f}')\n",
    "print(f'Recall:     {metrics.box.mr:.4f}')\n",
    "print(f'mAP50:      {metrics.box.map50:.4f}')\n",
    "print(f'mAP50-95:   {metrics.box.map:.4f}')\n",
    "print()\n",
    "print('üì• Downloading model...')\n",
    "print('Place this file at: models/yolov8_flood_highacc.pt in your FloodWatch project')\n",
    "\n",
    "files.download(output_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Also save the last.pt as backup ‚îÄ‚îÄ\n",
    "last_pt = f'{RUN_DIR}/weights/last.pt'\n",
    "if os.path.isfile(last_pt):\n",
    "    shutil.copy2(last_pt, 'yolov8_flood_highacc_last.pt')\n",
    "    print('Backup weights saved: yolov8_flood_highacc_last.pt')\n",
    "    # files.download('yolov8_flood_highacc_last.pt')  # Uncomment to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Export to ONNX (Optional)\n",
    "\n",
    "For faster CPU inference or edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export to ONNX\n",
    "# best_model.export(format='onnx', imgsz=IMG_SIZE)\n",
    "# onnx_path = f'{RUN_DIR}/weights/best.onnx'\n",
    "# if os.path.isfile(onnx_path):\n",
    "#     shutil.copy2(onnx_path, 'yolov8_flood_highacc.onnx')\n",
    "#     files.download('yolov8_flood_highacc.onnx')\n",
    "#     print(f'ONNX size: {os.path.getsize(\"yolov8_flood_highacc.onnx\") / 1024 / 1024:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}