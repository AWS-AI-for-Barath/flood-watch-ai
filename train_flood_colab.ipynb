{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodWatch YOLO - Maximum-Accuracy Training (YOLOv8l)\n",
    "\n",
    "End-to-end training pipeline for the highest-accuracy flood detection model.\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Model | YOLOv8l (43.7M params) |\n",
    "| Resolution | 832px |\n",
    "| Epochs | 250 (patience=50) |\n",
    "| Optimizer | AdamW (lr=0.002, wd=0.0005) |\n",
    "| Box loss | 7.5 (CIoU) |\n",
    "| Target | mAP50 >= 0.83-0.88 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics roboflow opencv-python pillow numpy tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(\"GPU:\", gpu_name, \"(\", round(vram_gb, 1), \"GB VRAM)\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# CONFIGURE THESE VALUES\n",
    "# ==============================================================\n",
    "\n",
    "# Roboflow settings\n",
    "ROBOFLOW_API_KEY = \"yUaG6RQfZ6ZrAFlwj6w7\"  # <-- ENTER YOUR API KEY\n",
    "ROBOFLOW_WORKSPACE = \"modellabel\"  # Add workspace name if known, else leave empty\n",
    "ROBOFLOW_PROJECT = \"yolo-floods-relief\"\n",
    "ROBOFLOW_VERSION = 1\n",
    "\n",
    "# Extra flood datasets (public Roboflow projects)\n",
    "EXTRA_PROJECTS = [\n",
    "    \"roboflow-universe-projects/flood-detection\",\n",
    "    \"roboflow-universe-projects/flood-area-detection\"\n",
    "]\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"./flood_dataset\"\n",
    "MERGED_DIR = \"./flood_merged\"\n",
    "\n",
    "# Classes\n",
    "CLASS_NAMES = [\"person\", \"car\", \"bicycle\", \"motorcycle\", \"bus\", \"truck\"]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "VALID_CLASS_IDS = set(range(NUM_CLASSES))\n",
    "\n",
    "# Training\n",
    "MODEL_NAME = \"yolov8l.pt\"\n",
    "IMG_SIZE = 832\n",
    "EPOCHS = 280\n",
    "PATIENCE = 60\n",
    "\n",
    "# Label cleaning\n",
    "MIN_BOX_AREA = 0.0005\n",
    "DUP_IOU_THRESH = 0.95\n",
    "TIGHTEN = 0.94  # shrink boxes 3% each side\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(\"  Model:\", MODEL_NAME)\n",
    "print(\"  Resolution:\", IMG_SIZE)\n",
    "print(\"  Epochs:\", EPOCHS, \"(patience\", str(PATIENCE) + \")\")\n",
    "print(\"  Classes:\", CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "if ROBOFLOW_WORKSPACE:\n",
    "    workspace = rf.workspace(ROBOFLOW_WORKSPACE)\n",
    "else:\n",
    "    workspace = rf.workspace()\n",
    "    \n",
    "project = workspace.project(ROBOFLOW_PROJECT)\n",
    "version = project.version(ROBOFLOW_VERSION)\n",
    "ds = version.download(\"yolov8\", location=BASE_DIR)\n",
    "\n",
    "print(\"Primary dataset downloaded to:\", BASE_DIR)\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = os.path.join(BASE_DIR, split, \"images\")\n",
    "    if os.path.isdir(img_dir):\n",
    "        n = len([f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        print(\"  \" + split + \":\", n, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download extra datasets\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "\n",
    "extra_dirs = []\n",
    "\n",
    "for i, proj_path in enumerate(EXTRA_PROJECTS):\n",
    "    loc = f\"./extra_flood_{i}\"\n",
    "    try:\n",
    "        proj = rf.project(proj_path)\n",
    "        \n",
    "        # Try common version numbers safely\n",
    "        version = None\n",
    "        for v in range(1, 10):\n",
    "            try:\n",
    "                version = proj.version(v)\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if version is None:\n",
    "            raise RuntimeError(\"No accessible version found\")\n",
    "            \n",
    "        version.download(\"yolov8\", location=loc)\n",
    "        extra_dirs.append(loc)\n",
    "        \n",
    "        print(f\"Downloaded: {proj_path} -> {loc}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {proj_path} -> {e}\")\n",
    "\n",
    "print(\"Extra datasets downloaded:\", len(extra_dirs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Labels (Polygon to BBox)\n",
    "\n",
    "Roboflow may export segmentation polygons. YOLO detect needs: `class xc yc w h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_to_bbox(parts):\n",
    "    \"\"\"Convert polygon annotation to YOLO bbox.\"\"\"\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    try:\n",
    "        cls_id = int(parts[0])\n",
    "        coords = [float(v) for v in parts[1:]]\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "    if len(coords) % 2 != 0:\n",
    "        return None\n",
    "    xs = coords[0::2]\n",
    "    ys = coords[1::2]\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "    x_min = max(0.0, min(xs))\n",
    "    x_max = min(1.0, max(xs))\n",
    "    y_min = max(0.0, min(ys))\n",
    "    y_max = min(1.0, max(ys))\n",
    "    w = x_max - x_min\n",
    "    h = y_max - y_min\n",
    "    if w <= 0 or h <= 0:\n",
    "        return None\n",
    "    xc = x_min + w / 2.0\n",
    "    yc = y_min + h / 2.0\n",
    "    return str(cls_id) + \" \" + \"{:.6f}\".format(xc) + \" \" + \"{:.6f}\".format(yc) + \" \" + \"{:.6f}\".format(w) + \" \" + \"{:.6f}\".format(h)\n",
    "\n",
    "\n",
    "def normalize_labels(dataset_dir):\n",
    "    \"\"\"Convert all polygon labels to bbox format in-place.\"\"\"\n",
    "    poly_total = 0\n",
    "    bbox_total = 0\n",
    "    fixed_files = 0\n",
    "    for split in [\"train\", \"valid\", \"test\", \"val\"]:\n",
    "        lbl_dir = os.path.join(dataset_dir, split, \"labels\")\n",
    "        if not os.path.isdir(lbl_dir):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(lbl_dir)):\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "            fpath = os.path.join(lbl_dir, fname)\n",
    "            with open(fpath, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            converted = []\n",
    "            had_poly = False\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 0:\n",
    "                    continue\n",
    "                if len(parts) == 5:\n",
    "                    converted.append(line.strip())\n",
    "                    bbox_total += 1\n",
    "                elif len(parts) > 5:\n",
    "                    b = polygon_to_bbox(parts)\n",
    "                    if b is not None:\n",
    "                        converted.append(b)\n",
    "                        poly_total += 1\n",
    "                        had_poly = True\n",
    "            if had_poly:\n",
    "                with open(fpath, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(converted) + \"\\n\")\n",
    "                fixed_files += 1\n",
    "    return poly_total, bbox_total, fixed_files\n",
    "\n",
    "\n",
    "# Normalize base dataset\n",
    "print(\"Normalizing base dataset...\")\n",
    "polys, bboxes, fixed = normalize_labels(BASE_DIR)\n",
    "print(\"  Polygons converted:\", polys)\n",
    "print(\"  Already bbox:\", bboxes)\n",
    "print(\"  Files fixed:\", fixed)\n",
    "\n",
    "# Normalize extras\n",
    "for d in extra_dirs:\n",
    "    print(\"Normalizing\", d, \"...\")\n",
    "    p, b, f2 = normalize_labels(d)\n",
    "    print(\"  Polygons converted:\", p)\n",
    "\n",
    "print(\"All labels normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean Labels and Tighten Bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(b1, b2):\n",
    "    \"\"\"IoU between two boxes: (cls, xc, yc, w, h).\"\"\"\n",
    "    _, x1, y1, w1, h1 = b1\n",
    "    _, x2, y2, w2, h2 = b2\n",
    "    ax1 = x1 - w1 / 2.0\n",
    "    ay1 = y1 - h1 / 2.0\n",
    "    ax2 = x1 + w1 / 2.0\n",
    "    ay2 = y1 + h1 / 2.0\n",
    "    bx1 = x2 - w2 / 2.0\n",
    "    by1 = y2 - h2 / 2.0\n",
    "    bx2 = x2 + w2 / 2.0\n",
    "    by2 = y2 + h2 / 2.0\n",
    "    ix1 = max(ax1, bx1)\n",
    "    iy1 = max(ay1, by1)\n",
    "    ix2 = min(ax2, bx2)\n",
    "    iy2 = min(ay2, by2)\n",
    "    if ix2 <= ix1 or iy2 <= iy1:\n",
    "        return 0.0\n",
    "    inter = (ix2 - ix1) * (iy2 - iy1)\n",
    "    union = w1 * h1 + w2 * h2 - inter\n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def clean_and_tighten(dataset_dir):\n",
    "    \"\"\"Clean labels: remove bad/tiny/duplicate annotations, tighten boxes.\"\"\"\n",
    "    stats = defaultdict(int)\n",
    "    for split in [\"train\", \"valid\", \"test\", \"val\"]:\n",
    "        lbl_dir = os.path.join(dataset_dir, split, \"labels\")\n",
    "        if not os.path.isdir(lbl_dir):\n",
    "            continue\n",
    "        label_files = sorted([f for f in os.listdir(lbl_dir) if f.endswith(\".txt\")])\n",
    "        for fname in tqdm(label_files, desc=\"Cleaning \" + split):\n",
    "            fpath = os.path.join(lbl_dir, fname)\n",
    "            with open(fpath, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            stats[\"total_files\"] += 1\n",
    "            anns = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    stats[\"bad_format\"] += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                    xc = float(parts[1])\n",
    "                    yc = float(parts[2])\n",
    "                    w = float(parts[3])\n",
    "                    h = float(parts[4])\n",
    "                except ValueError:\n",
    "                    stats[\"parse_error\"] += 1\n",
    "                    continue\n",
    "                # Validate class\n",
    "                if cls not in VALID_CLASS_IDS:\n",
    "                    stats[\"bad_class\"] += 1\n",
    "                    continue\n",
    "                # Clamp center\n",
    "                xc = max(0.0, min(1.0, xc))\n",
    "                yc = max(0.0, min(1.0, yc))\n",
    "                # Fix edges\n",
    "                if xc - w / 2.0 < 0:\n",
    "                    w = xc * 2.0\n",
    "                if xc + w / 2.0 > 1:\n",
    "                    w = (1.0 - xc) * 2.0\n",
    "                if yc - h / 2.0 < 0:\n",
    "                    h = yc * 2.0\n",
    "                if yc + h / 2.0 > 1:\n",
    "                    h = (1.0 - yc) * 2.0\n",
    "                # Remove tiny\n",
    "                if w * h < MIN_BOX_AREA:\n",
    "                    stats[\"tiny\"] += 1\n",
    "                    continue\n",
    "                # Tighten\n",
    "                w = w * TIGHTEN\n",
    "                h = h * TIGHTEN\n",
    "                w = max(w, 0.005)\n",
    "                h = max(h, 0.005)\n",
    "                anns.append((cls, xc, yc, w, h))\n",
    "            # Remove duplicates\n",
    "            final = []\n",
    "            for a in anns:\n",
    "                is_dup = False\n",
    "                for e in final:\n",
    "                    if a[0] == e[0] and calc_iou(a, e) > DUP_IOU_THRESH:\n",
    "                        is_dup = True\n",
    "                        break\n",
    "                if is_dup:\n",
    "                    stats[\"duplicate\"] += 1\n",
    "                else:\n",
    "                    final.append(a)\n",
    "            # Write\n",
    "            if len(final) == 0:\n",
    "                stats[\"empty\"] += 1\n",
    "                # Keep file but empty (hard negative)\n",
    "                with open(fpath, \"w\") as f:\n",
    "                    f.write(\"\")\n",
    "            else:\n",
    "                with open(fpath, \"w\") as f:\n",
    "                    for cls, xc, yc, w, h in final:\n",
    "                        line_out = str(cls) + \" \" + \"{:.6f}\".format(xc) + \" \" + \"{:.6f}\".format(yc) + \" \" + \"{:.6f}\".format(w) + \" \" + \"{:.6f}\".format(h) + \"\\n\"\n",
    "                        f.write(line_out)\n",
    "                stats[\"clean_annotations\"] += len(final)\n",
    "    return dict(stats)\n",
    "\n",
    "\n",
    "print(\"Cleaning base dataset...\")\n",
    "report = clean_and_tighten(BASE_DIR)\n",
    "print(\"\")\n",
    "print(\"Cleaning report:\")\n",
    "for k in sorted(report.keys()):\n",
    "    print(\"  \" + k + \": \" + str(report[k]))\n",
    "\n",
    "for d in extra_dirs:\n",
    "    print(\"Cleaning\", d, \"...\")\n",
    "    clean_and_tighten(d)\n",
    "\n",
    "print(\"Labels cleaned and tightened.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_into(src_dir, dst_dir, prefix, class_map=None):\n",
    "    \"\"\"Copy images and labels from src into dst, with optional class ID mapping.\"\"\"\n",
    "    copied = 0\n",
    "    for split in [\"train\", \"valid\", \"test\", \"val\"]:\n",
    "        dst_split = \"valid\" if split == \"val\" else split\n",
    "        src_img_dir = os.path.join(src_dir, split, \"images\")\n",
    "        src_lbl_dir = os.path.join(src_dir, split, \"labels\")\n",
    "        dst_img_dir = os.path.join(dst_dir, dst_split, \"images\")\n",
    "        dst_lbl_dir = os.path.join(dst_dir, dst_split, \"labels\")\n",
    "        if not os.path.isdir(src_img_dir):\n",
    "            continue\n",
    "        os.makedirs(dst_img_dir, exist_ok=True)\n",
    "        os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "        for fname in os.listdir(src_img_dir):\n",
    "            lower = fname.lower()\n",
    "            if not (lower.endswith(\".jpg\") or lower.endswith(\".jpeg\") or lower.endswith(\".png\")):\n",
    "                continue\n",
    "            dst_fname = prefix + fname\n",
    "            # Copy image\n",
    "            src_path = os.path.join(src_img_dir, fname)\n",
    "            dst_path = os.path.join(dst_img_dir, dst_fname)\n",
    "            if not os.path.exists(dst_path):\n",
    "                import shutil\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "            # Copy label\n",
    "            base_name = os.path.splitext(fname)[0]\n",
    "            dst_base = os.path.splitext(dst_fname)[0]\n",
    "            src_lbl = os.path.join(src_lbl_dir, base_name + \".txt\")\n",
    "            dst_lbl = os.path.join(dst_lbl_dir, dst_base + \".txt\")\n",
    "            \n",
    "            if os.path.isfile(src_lbl) and not os.path.exists(dst_lbl):\n",
    "                if class_map is not None:\n",
    "                    with open(src_lbl, \"r\") as f:\n",
    "                        lines = f.readlines()\n",
    "                    mapped_lines = []\n",
    "                    for line in lines:\n",
    "                        p = line.strip().split()\n",
    "                        if len(p) == 5:\n",
    "                            old_id = int(p[0])\n",
    "                            new_id = class_map.get(old_id, None)\n",
    "                            if new_id is not None:\n",
    "                                mapped_line = str(new_id) + \" \" + \" \".join(p[1:])\n",
    "                                mapped_lines.append(mapped_line)\n",
    "                    with open(dst_lbl, \"w\") as f:\n",
    "                        f.write(\"\\n\".join(mapped_lines) + \"\\n\")\n",
    "                else:\n",
    "                    import shutil\n",
    "                    shutil.copy2(src_lbl, dst_lbl)\n",
    "            elif not os.path.exists(dst_lbl):\n",
    "                open(dst_lbl, \"w\").close()\n",
    "            copied += 1\n",
    "    return copied\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "# Remove old merged dir\n",
    "if os.path.isdir(MERGED_DIR):\n",
    "    shutil.rmtree(MERGED_DIR)\n",
    "\n",
    "# Copy base\n",
    "print(\"Merging base dataset...\")\n",
    "n = merge_into(BASE_DIR, MERGED_DIR, \"\")\n",
    "print(\"  Base:\", n, \"files\")\n",
    "\n",
    "# Manual class mappings to map extra datasets to FloodWatch schema:\n",
    "# 0: person, 1: car, 2: bicycle, 3: motorcycle, 4: bus, 5: truck\n",
    "# For 'flood-segmentation-gsmmc' (assumes 0:person, 1:car/vehicle, etc)\n",
    "# This mapping safely ignores unrelated classes by mapping to None\n",
    "x_maps = [\n",
    "    {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}, # Fallback exact mapping\n",
    "    {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}, \n",
    "]\n",
    "\n",
    "# Copy extras\n",
    "for i, d in enumerate(extra_dirs):\n",
    "    prefix = \"x\" + str(i) + \"_\"\n",
    "    cmap = x_maps[i] if i < len(x_maps) else None\n",
    "    n = merge_into(d, MERGED_DIR, prefix, class_map=cmap)\n",
    "    print(\"  Extra\", i, \":\", n, \"files\")\n",
    "\n",
    "# Count\n",
    "total_images = 0\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = os.path.join(MERGED_DIR, split, \"images\")\n",
    "    if os.path.isdir(img_dir):\n",
    "        count = len([f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        total_images += count\n",
    "        print(\"  \" + split + \":\", count, \"images\")\n",
    "print(\"  Total:\", total_images, \"images merged.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Balance Classes\n",
    "\n",
    "Oversample minority classes to ~70% of the majority class count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_dir = os.path.join(MERGED_DIR, \"train\", \"labels\")\n",
    "img_dir = os.path.join(MERGED_DIR, \"train\", \"images\")\n",
    "\n",
    "# Count classes\n",
    "counts = Counter()\n",
    "cls_files = defaultdict(set)\n",
    "for fname in os.listdir(lbl_dir):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "    fpath = os.path.join(lbl_dir, fname)\n",
    "    with open(fpath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                cls = int(parts[0])\n",
    "                counts[cls] += 1\n",
    "                cls_files[cls].add(fname)\n",
    "\n",
    "print(\"Before balancing:\")\n",
    "for c in sorted(counts.keys()):\n",
    "    name = CLASS_NAMES[c] if c < len(CLASS_NAMES) else str(c)\n",
    "    n_imgs = len(cls_files[c])\n",
    "    print(\"  Class\", c, \"(\" + name + \"):\", counts[c], \"annotations,\", n_imgs, \"images\")\n",
    "\n",
    "if len(counts) > 0:\n",
    "    max_count = max(counts.values())\n",
    "    target = int(max_count * 0.7)\n",
    "    total_added = 0\n",
    "    for cls_id in sorted(counts.keys()):\n",
    "        if counts[cls_id] >= target:\n",
    "            continue\n",
    "        files = sorted(list(cls_files.get(cls_id, set())))\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "        needed = target - counts[cls_id]\n",
    "        max_dups = min(needed, len(files) * 3)\n",
    "        for i in range(max_dups):\n",
    "            src_lbl_name = files[i % len(files)]\n",
    "            base = os.path.splitext(src_lbl_name)[0]\n",
    "            dup_base = base + \"_bal\" + str(i)\n",
    "            dst_lbl_path = os.path.join(lbl_dir, dup_base + \".txt\")\n",
    "            if os.path.exists(dst_lbl_path):\n",
    "                continue\n",
    "            # Copy label\n",
    "            src_lbl_path = os.path.join(lbl_dir, src_lbl_name)\n",
    "            shutil.copy2(src_lbl_path, dst_lbl_path)\n",
    "            # Find and copy+flip image\n",
    "            for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                src_img_path = os.path.join(img_dir, base + ext)\n",
    "                if os.path.isfile(src_img_path):\n",
    "                    dst_img_path = os.path.join(img_dir, dup_base + ext)\n",
    "                    try:\n",
    "                        img = Image.open(src_img_path)\n",
    "                        flipped = ImageOps.mirror(img)\n",
    "                        flipped.save(dst_img_path)\n",
    "                        # Flip x coords in label\n",
    "                        with open(dst_lbl_path, \"r\") as f:\n",
    "                            lbl_lines = f.readlines()\n",
    "                        new_lines = []\n",
    "                        for ll in lbl_lines:\n",
    "                            pp = ll.strip().split()\n",
    "                            if len(pp) == 5:\n",
    "                                flipped_x = 1.0 - float(pp[1])\n",
    "                                new_line = pp[0] + \" \" + \"{:.6f}\".format(flipped_x) + \" \" + pp[2] + \" \" + pp[3] + \" \" + pp[4]\n",
    "                                new_lines.append(new_line)\n",
    "                        with open(dst_lbl_path, \"w\") as f:\n",
    "                            f.write(\"\\n\".join(new_lines) + \"\\n\")\n",
    "                    except Exception as ex:\n",
    "                        shutil.copy2(src_img_path, dst_img_path)\n",
    "                    break\n",
    "            total_added += 1\n",
    "    print(\"\")\n",
    "    print(\"Oversampled\", total_added, \"images\")\n",
    "\n",
    "# Recount\n",
    "new_counts = Counter()\n",
    "for fname in os.listdir(lbl_dir):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "    with open(os.path.join(lbl_dir, fname), \"r\") as f:\n",
    "        for line in f:\n",
    "            pp = line.strip().split()\n",
    "            if len(pp) == 5:\n",
    "                new_counts[int(pp[0])] += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"After balancing:\")\n",
    "for c in sorted(new_counts.keys()):\n",
    "    name = CLASS_NAMES[c] if c < len(CLASS_NAMES) else str(c)\n",
    "    print(\"  Class\", c, \"(\" + name + \"):\", new_counts[c], \"annotations\")\n",
    "\n",
    "print(\"Classes balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_merged = os.path.abspath(MERGED_DIR)\n",
    "\n",
    "data_config = {\n",
    "    \"path\": abs_merged,\n",
    "    \"train\": \"train/images\",\n",
    "    \"val\": \"valid/images\",\n",
    "    \"nc\": NUM_CLASSES,\n",
    "    \"names\": CLASS_NAMES,\n",
    "}\n",
    "\n",
    "test_dir = os.path.join(MERGED_DIR, \"test\", \"images\")\n",
    "if os.path.isdir(test_dir):\n",
    "    data_config[\"test\"] = \"test/images\"\n",
    "\n",
    "yaml_path = os.path.join(MERGED_DIR, \"data.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"Generated data.yaml at:\", yaml_path)\n",
    "print(\"\")\n",
    "with open(yaml_path, \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train YOLOv8l - Maximum Accuracy\n",
    "\n",
    "Training with:\n",
    "- AdamW optimizer (lr=0.002, cosine schedule)\n",
    "- Box loss weight 7.5 (CIoU localization emphasis)\n",
    "- Full augmentations: mosaic, mixup, HSV, scale, perspective, flips, copy-paste, erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"  Model:\", MODEL_NAME)\n",
    "print(\"  Data:\", yaml_path)\n",
    "print(\"  Image size:\", IMG_SIZE)\n",
    "print(\"  Epochs:\", EPOCHS)\n",
    "\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=-1,\n",
    "    patience=PATIENCE,\n",
    "    device=0,\n",
    "    project=\"./runs\",\n",
    "    name=\"flood_maxacc_l\",\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr0=0.0015,\n",
    "    lrf=0.01,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=5,\n",
    "    warmup_momentum=0.8,\n",
    "    cos_lr=True,\n",
    "    mosaic=1.0,\n",
    "    mixup=0.1,\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    scale=0.5,\n",
    "    translate=0.1,\n",
    "    perspective=0.0005,\n",
    "    flipud=0.2,\n",
    "    fliplr=0.5,\n",
    "    degrees=5.0,\n",
    "    shear=2.0,\n",
    "    copy_paste=0.1,\n",
    "    erasing=0.1,\n",
    "    box=8.5,\n",
    "    cls=0.5,\n",
    "    dfl=1.5,\n",
    "    save=True,\n",
    "    save_period=25,\n",
    "    plots=True,\n",
    "    val=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MAP50 = 0.69\n",
    "\n",
    "metrics = {}\n",
    "if hasattr(results, \"results_dict\"):\n",
    "    metrics = results.results_dict\n",
    "\n",
    "precision = metrics.get(\"metrics/precision(B)\", None)\n",
    "recall = metrics.get(\"metrics/recall(B)\", None)\n",
    "map50 = metrics.get(\"metrics/mAP50(B)\", None)\n",
    "map50_95 = metrics.get(\"metrics/mAP50-95(B)\", None)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  FloodWatch YOLO - Results\")\n",
    "print(\"=\" * 55)\n",
    "print(\"  Model:     \", MODEL_NAME)\n",
    "print(\"  Resolution:\", IMG_SIZE, \"px\")\n",
    "print(\"  Epochs:    \", EPOCHS)\n",
    "print(\"\")\n",
    "print(\"  Precision: \", precision)\n",
    "print(\"  Recall:    \", recall)\n",
    "print(\"  mAP50:     \", map50)\n",
    "print(\"  mAP50-95:  \", map50_95)\n",
    "print(\"\")\n",
    "\n",
    "if map50 is not None:\n",
    "    delta = map50 - BASELINE_MAP50\n",
    "    if delta > 0:\n",
    "        status = \"IMPROVED\"\n",
    "    else:\n",
    "        status = \"REGRESSION\"\n",
    "    print(\"  Baseline mAP50:\", BASELINE_MAP50)\n",
    "    print(\"  Delta:         \", round(delta, 4), \"(\", status, \")\")\n",
    "    if map50 >= 0.83:\n",
    "        print(\"\")\n",
    "        print(\"  TARGET ACHIEVED! mAP50 >= 0.83\")\n",
    "    elif map50 >= 0.80:\n",
    "        print(\"\")\n",
    "        print(\"  Near target: mAP50 >= 0.80\")\n",
    "\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pt = \"./runs/flood_maxacc_l/weights/best.pt\"\n",
    "last_pt = \"./runs/flood_maxacc_l/weights/last.pt\"\n",
    "export_name = \"yolov8_flood_highacc.pt\"\n",
    "\n",
    "if os.path.isfile(best_pt):\n",
    "    shutil.copy2(best_pt, export_name)\n",
    "    size_mb = os.path.getsize(export_name) / 1e6\n",
    "    print(\"Exported:\", export_name, \"(\", round(size_mb, 1), \"MB)\")\n",
    "elif os.path.isfile(last_pt):\n",
    "    shutil.copy2(last_pt, export_name)\n",
    "    print(\"Using last.pt as\", export_name)\n",
    "else:\n",
    "    print(\"WARNING: No weights found!\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Place this file at: flood-watch-ai/models/yolov8_flood_highacc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training plots\n",
    "plots_dir = \"./runs/flood_maxacc_l\"\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "for plot_name in [\"results.png\", \"confusion_matrix.png\", \"PR_curve.png\", \"F1_curve.png\"]:\n",
    "    plot_path = os.path.join(plots_dir, plot_name)\n",
    "    if os.path.isfile(plot_path):\n",
    "        print(plot_name + \":\")\n",
    "        display(IPImage(filename=plot_path, width=800))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model to your computer\n",
    "try:\n",
    "    from google.colab import files\n",
    "    if os.path.isfile(export_name):\n",
    "        files.download(export_name)\n",
    "        print(\"Downloading\", export_name, \"...\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab. Copy\", export_name, \"manually.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}